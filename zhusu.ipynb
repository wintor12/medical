{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled with initial size: 80.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import jieba\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.358 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 1/ / 甲状腺/ 肝功/ 亢进/ 亢进症/ 并/ 肝/ 损伤/ 2/ / 发热/ 原因/ 待查/ 尿路/ 尿路感染/ 感染/ / / 淋巴/ 淋巴瘤/ / / \n",
      "Default Mode: 1/ ./ 甲状腺/ 肝功/ 亢进症/ 并/ 肝/ 损伤/ 2/ ./ 发热/ 原因/ 待查/ 尿路感染/ ？/ 淋巴瘤/ ？/ )\n"
     ]
    }
   ],
   "source": [
    "st = '1.甲状腺肝功亢进症并肝损伤2.发热原因待查尿路感染？淋巴瘤？)'\n",
    "\n",
    "seg_list = jieba.cut(st, cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(st, cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/home/tongwang/data/medical/2013年主诉.csv') as p:\n",
    "    s = p.read()\n",
    "lines = s.split('\\n')\n",
    "lines = [line[1:-2] for line in lines]\n",
    "sentences=['.'.join(sent.split('\\t')[1:]) for sent in lines]\n",
    "sentences=[list(jieba.cut(sent, cut_all=False)) for sent in sentences]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss=[]\n",
    "for sentence in sentences:\n",
    "    s = []\n",
    "    for word in sentence:\n",
    "        s.append(word.encode('utf-8'))\n",
    "    ss.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-09-23 12:23:36,027 : INFO : collecting all words and their counts\n",
      "2016-09-23 12:23:36,029 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-09-23 12:23:36,080 : INFO : PROGRESS: at sentence #10000, processed 157572 words, keeping 4690 word types\n",
      "2016-09-23 12:23:36,134 : INFO : PROGRESS: at sentence #20000, processed 314075 words, keeping 6017 word types\n",
      "2016-09-23 12:23:36,175 : INFO : PROGRESS: at sentence #30000, processed 472473 words, keeping 7070 word types\n",
      "2016-09-23 12:23:36,218 : INFO : PROGRESS: at sentence #40000, processed 630177 words, keeping 7791 word types\n",
      "2016-09-23 12:23:36,263 : INFO : PROGRESS: at sentence #50000, processed 787482 words, keeping 8492 word types\n",
      "2016-09-23 12:23:36,302 : INFO : PROGRESS: at sentence #60000, processed 946469 words, keeping 9104 word types\n",
      "2016-09-23 12:23:36,328 : INFO : collected 9380 word types from a corpus of 1032189 raw words and 65537 sentences\n",
      "2016-09-23 12:23:36,356 : INFO : min_count=1 retains 9380 unique words (drops 0)\n",
      "2016-09-23 12:23:36,359 : INFO : min_count leaves 1032189 word corpus (100% of original 1032189)\n",
      "2016-09-23 12:23:36,404 : INFO : deleting the raw counts dictionary of 9380 items\n",
      "2016-09-23 12:23:36,405 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2016-09-23 12:23:36,406 : INFO : downsampling leaves estimated 659847 word corpus (63.9% of prior 1032189)\n",
      "2016-09-23 12:23:36,406 : INFO : estimated required memory for 9380 words and 100 dimensions: 12194000 bytes\n",
      "2016-09-23 12:23:36,438 : INFO : resetting layer weights\n",
      "2016-09-23 12:23:36,641 : INFO : training model with 3 workers on 9380 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5\n",
      "2016-09-23 12:23:36,641 : INFO : expecting 65537 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-09-23 12:23:37,652 : INFO : PROGRESS: at 33.67% examples, 1110154 words/s, in_qsize 5, out_qsize 0\n",
      "2016-09-23 12:23:38,654 : INFO : PROGRESS: at 68.13% examples, 1121790 words/s, in_qsize 6, out_qsize 0\n",
      "2016-09-23 12:23:39,608 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-09-23 12:23:39,612 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-09-23 12:23:39,613 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-09-23 12:23:39,614 : INFO : training on 5160945 raw words (3299279 effective words) took 3.0s, 1113056 effective words/s\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "头晕 0.888966798782\n",
      "鼻塞 0.737387299538\n",
      "四肢无力 0.736973643303\n",
      "头疼 0.724947929382\n",
      "呕吐 0.715142726898\n",
      "抽搐 0.692003965378\n",
      "眩晕 0.685406625271\n",
      "不清 0.677242636681\n",
      "恶心 0.676798582077\n",
      "耳鸣 0.673804998398\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'头痛')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呕吐 0.843178629875\n",
      "腹胀 0.831029236317\n",
      "便血 0.812211036682\n",
      "黑便 0.809713482857\n",
      "呃逆 0.784659743309\n",
      "纳差 0.779944658279\n",
      "呕血 0.775841832161\n",
      "发热 0.767299711704\n",
      "腹痛 0.756284236908\n",
      "低热 0.750950336456\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'腹泻')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呕吐 0.853747367859\n",
      "反酸 0.748754620552\n",
      "腹泻 0.746998906136\n",
      "干呕 0.746487498283\n",
      "纳差 0.733575284481\n",
      "呃逆 0.716074049473\n",
      "乏力 0.69236779213\n",
      "腹胀 0.684635579586\n",
      "头疼 0.680924832821\n",
      "烧心 0.6800096035\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'恶心')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肋骨 0.789254784584\n",
      "股骨 0.761635541916\n",
      "粉碎性 0.759858727455\n",
      "伤 0.744992136955\n",
      "固定 0.730760872364\n",
      "喙 0.724349081516\n",
      "挫灭 0.717911064625\n",
      "桡骨 0.71576499939\n",
      "挫伤 0.71266746521\n",
      "髁 0.708834946156\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'骨折')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结肠 0.885604798794\n",
      "乙状结肠 0.828190267086\n",
      "胃窦 0.756129980087\n",
      "贲门 0.749992072582\n",
      "胃 0.749833881855\n",
      "十二指肠 0.749315738678\n",
      "肛管 0.682750165462\n",
      "胰腺 0.681108474731\n",
      "大肠 0.675479888916\n",
      "胰头 0.660688042641\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'直肠')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "胰腺炎 0.822397947311\n",
      "胃炎 0.810499429703\n",
      "胆囊结石 0.753069043159\n",
      "阑尾炎 0.74833714962\n",
      "胆管炎 0.747984290123\n",
      "食管炎 0.702635407448\n",
      "脂肪肝 0.668958604336\n",
      "胃溃疡 0.66607105732\n",
      "肾盂肾炎 0.65399324894\n",
      "胃肠炎 0.644857466221\n"
     ]
    }
   ],
   "source": [
    "words = model.most_similar(u'胆囊炎')\n",
    "for word in words:\n",
    "    print word[0],word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = model.vocab.keys()\n",
    "res = []\n",
    "for x in vocab:\n",
    "    words = model.most_similar(x)\n",
    "    words = [y[0] for y in words]\n",
    "    res.append(x + ',' + ','.join(words))\n",
    "\n",
    "with open('test.txt', 'w') as p:\n",
    "    p.write('\\n'.join(res).encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
